# **Predictive CI/CD Pipeline Optimizer**

This project is a complete simulation of an AIOps system designed to predict CI build failures before running a time-consuming test suite.

It uses a machine learning model (trained on historical build data) to make a real-time prediction on a new commit.

# Project Components:

`gui_app.py`: (NEW) A graphical interface to interactively test predictions with a visual dashboard.

`generate_data.py`: A script to create a fake build_history.csv. This simulates your historical build log with realistic patterns (e.g., large commits are riskier).

`train_model.py`: Reads build_history.csv, trains a RandomForestClassifier to predict build_status, and saves the final ci_model.pkl and model_features.json.

`run_ci_pipeline.py`: A script that simulates a real CI run. It takes commit info (like author, files changed) as arguments, loads the model, and makes a prediction.

`visualize_analysis.py`: A standalone script that loads the model and data to generate diagnostic charts (Confusion Matrix, Feature Importance, etc.) into an images/ folder.

requirements.txt: Python dependencies.

# How to Run This Demo

# Set up your environment:

# Create a virtual environment (recommended)
'python -m venv venv
source venv/bin/activate 

# Install dependencies
pip install -r requirements.txt


# Step 1: Generate Historical Data
Run the data generator. This only needs to be done once.

python generate_data.py


You will see output as it creates build_history.csv.

# Step 2: Train the AI Model
Run the training script. This will read the CSV and create two new files: ci_model.pkl and model_features.json.

python train_model.py


You will see the model's accuracy and a classification report.

# Step 3: Visualize the Model Logic (Optional)
Run the analysis script to generate charts explaining how the model works.

python visualize_analysis.py


This will create an images/ folder containing PNG charts for Feature Importance, Confusion Matrix, and Failure Trends.

# Step 4: Run the Prediction Dashboard (GUI)
Launch the graphical interface to test the model interactively.

python gui_app.py

**Scenario A (Safe)**: Select "main_dev", ".py", and low changes. Click "Analyze". The result should be GREEN.

**Scenario B (Risky)**: Select "new_dev", ".yml", and high changes. Click "Analyze". The result should be RED.

**How to Integrate This into a REAL System (e.g., GitHub Actions)**

This simulation shows the core logic. Here is the architectural plan to implement this for real.

# Phase 1: Data Collection (Run Asynchronously)

You need to start logging your build history to a central database.

Database: Create a database (e.g., PostgreSQL, AWS S3 bucket) to store build results.

Modify CI Pipeline: At the end of your existing CI pipeline (after tests run), add a new step.

This step should:

Gather metadata: git log -1 --pretty=format:'%an', git diff --shortstat, build status (success/failure).

Run a Python script to send this data to your database.

# Phase 2: Model Training (Run on a Schedule)

The model should be re-trained periodically to learn from new data.

Scheduled Job: Create a scheduled task (e.g., a nightly cron job or a schedule trigger in GitHub Actions).

This job should:

Run train_model.py.

Modify the script to read from your database, not a CSV.

Instead of saving the model locally, it should save ci_model.pkl and model_features.json to a central artifact store (like an S3 bucket or GitHub Packages).

# Phase 3: Prediction/Inference (Run in Real-Time)

This is where you modify your main CI pipeline to use the model.

New First Step: Add a new step at the very beginning of your CI pipeline, before your npm test or pytest command.

This step should:

Download the latest ci_model.pkl and model_features.json from your S3 bucket.

Gather the metadata for the current commit (author, files changed, etc.).

Run run_ci_pipeline.py with this metadata.

# The "Smart" Logic:

The run_ci_pipeline.py script will exit with 0 (success) or 1 (failure).

In your CI pipeline, check this exit code. If the exit code is 1 (predict failure), fail the entire pipeline immediately.

If the exit code is 0 (predict success), allow the pipeline to continue to the "long test suite" step as normal.

This architecture separates data collection, model training, and prediction, which is the core principle of a robust MLOps/AIOps system.